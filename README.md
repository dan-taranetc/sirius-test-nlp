# NLP Кейс Сириус

Этот репозиторий содержит решение NLP кейса для смены по машинному обучению от Тинькофф в Университете "Сириус".

---
# Решение
 
## Выбор данных и файнтюнинг модели

За основу была взята рекомендованная модель [ruDialoGPT-medium](https://huggingface.co/tinkoff-ai/ruDialoGPT-medium). 
При тестовом запуске модели было замечено, что модель не способна отвечать на сообщения, учитывая контекст предыдущей переписки. 
Поэтому основной целью, которую я преследовал при обучении модели, стало сделать **ruDialoGPT-medium** способной учитывать предыдущие сообщения при ответе пользователю.

Для дообучения были выбраны открытые чаты, с большим количеством сообщений. Преимущественно это чаты с IT тематикой, но мне также показалось интересным добавить
некоторое разнообразие, чтобы диалоговая модель не была зациклена на одной теме:
1. [DevOps - русско-говорящее сообщество](https://telegram.me/devops_ru)
2. [Go-Go!](https://t.me/gogolang)
3. [CyberJobsRussia](https://t.me/CyberJobsRussia)
4. [SEO chat true Original 100%](https://telegram.me/seochat)
5. [Startup Community](https://telegram.me/startupchat)

Выкачанные исходники в формате **json** можно найти [тут](https://drive.google.com/drive/folders/1jsXSPtLN_oVKsrRvs0PAqAGGU33NMLw7?usp=sharing).

С помощью скрипта `prepare_messages.py` данные были преобразованы в **csv** формат (можно найти [тут](https://drive.google.com/drive/folders/1daAB6NxtnodAn73OLhEeHZJLPbJ3Ztez?usp=sharing)), чтобы в дальнейшем предобработать их и дообучить модель.
Итоговый датасет состоит из ~ 600 тысяч строк, содержащих 3 сообщения контекста и ответ. Дообучение модели происходило на вычислительном кластере 
с A100, после чего модель была загружена на **huggging-face**: [ruDialoGPT_v2_medium](https://huggingface.co/taranetsdan/ruDialoGPT_v2_medium).

Папка [finetuning](/finetuning) содержит все скрипты для повторения результатов:
* `config.py` - файл, содержащий параметры загрузки данных, обучения модели и креды hf;
* `finetuning.py` - основной скрипт, в котором происходит предобратка данных и обучение модели;
* `train.sh` - скрипт для запуска процесса файнтюнинга на кластере с использованием менеджера очередей **slurm**.

Для запуска необходимо: скачать [сообщения, преобразованные в **csv** формат](https://drive.google.com/drive/folders/1daAB6NxtnodAn73OLhEeHZJLPbJ3Ztez?usp=sharing), и поместить их в папку [data_output](/finetuning/data_output). 
Затем заполнить `config.py` нужными параметрами и запустить установку зависимостей и скрипт вручную:
```bash
python3 -m pip install -r requierements.txt && python3 finetuning.py
```
или с использованием менеджера очередей **slurm**:
```bash
sbatch train.sh
```
---
## Бот и сервис для инференса модели

Бот, сервис для инференса и база данных реализованы как 3 **docker** контейнера, разворачивающиеся в одной 
сети с помощью `docker-compose.yaml`

### Сервис
Инференс модели реализован на **FastApi**. Код сервиса можно найти в папке [model_server](/model_server). Предобученная модель 
с помощью **git-lfs** устанавливается в контейнер и затем разворачивается через библиотеку **transformers**. После
загрузки обращение к модели доступно внутри сети по адресу: `http://dialogpt_server:80/answer`

### Бот 
Телеграмм-бот на **aiogram**, способный отвечать на сообщения пользователя, находится в папке [bot](/bot). 

Помимо обычных текстовых сообщений бот также поддерживает отправку голосовых сообщений, которые в дальнейшем расшифровываются с помощью 
библиотеки **SpeachRecognition** и уже в текстовом виде отправляются модели.  

Также бот хранит последние 3 сообщения в базе данных и по команде `/clear_context` способен очистить контекст,
чтобы начать диалог с чистого листа. Интеграция с базой данных реализованая через **SQLAlchemy** драйвер.

Поддерживаемые команды:
* `/start` - начать диалог с ботом;
* `/help` - получить help-message от бота;
* `/clear_context` - очистить контекст переписки;
* `{any text_message}` - получить ответ модели на сообщение;
* `{any voice_message}` - получить ответ модели на голосовое сообщение;

### База данных
Для хранения контекста переписки была выбрана база данных **postgres**. В ней хранится **id** пользователя
и контекст в формате **JSONB**.

Для того чтобы развернуть инфраструктуру, состоящую из бота, сервиса для инференса и базы данных
необходимо иметь поднятый **docker deamon**. Код для запуска:
```bash
BOT_TOKEN=... DB_USER=... DB_PASSWORD=... docker-compose up --build
```
Здесь:
* **BOT_TOKEN** - уникальный api токен бота, который можно получить у [BotFather](https://t.me/BotFather);
* **DB_USER** - имя пользователя, который будет создан в базе данных;
* **DB_PASSWORD** - пароль пользователя в базе данных.
